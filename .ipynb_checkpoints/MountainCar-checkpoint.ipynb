{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode:   13 Total episode reward: 54.3269 Learning: False Mean Action: 0.054 Std Action: 0.115 memorylen: 50000 0 \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing important things\n",
    "from DDPG import DDPG\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from OUNoise import OUNoise\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_state(state):\n",
    "    s = np.array(state)\n",
    "    s[0] = state[0] * 10\n",
    "    s[1] = state[1] * 60\n",
    "    return s\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env = env.unwrapped\n",
    "action_low = -1\n",
    "action_high = 1\n",
    "buffer_size = 25000\n",
    "batch_size = 1024\n",
    "agent = DDPG(action_low, action_high, buffer_size, batch_size)\n",
    "noise = OUNoise()\n",
    "# np.random.seed(2018)\n",
    "learning_epochs = int(buffer_size)\n",
    "episodes = 1500\n",
    "epochs = 1500\n",
    "epsilon = 1\n",
    "total_epochs = 0\n",
    "start_act = 30\n",
    "learning = True\n",
    "acting = False\n",
    "stuck = False\n",
    "done = False\n",
    "stop_train = False\n",
    "final_flag = True\n",
    "rewards_hist = []\n",
    "last_reset = 0\n",
    "max_reward = -np.inf\n",
    "last_max = 0\n",
    "\n",
    "print('Start learning phase...')\n",
    "for i_episode in range(1, episodes):\n",
    "    mem_len = agent.memory.__len__()\n",
    "    mem_full = mem_len >= buffer_size\n",
    "    if mem_len >= learning_epochs:\n",
    "        learning = False\n",
    "    if i_episode > start_act:\n",
    "        acting = True\n",
    "\n",
    "    \n",
    "    if done and acting:\n",
    "        # epochs = max(epochs-15, 200)\n",
    "        epsilon = max(epsilon-0.05, 0)\n",
    "    else:\n",
    "        # epochs = min(epochs+15, 3000)\n",
    "        epsilon = min(epsilon+0.05, 1)\n",
    "        if stuck > 20 and i_episode > (last_reset+5*buffer_size/epochs):\n",
    "            print('Stuck!! Reseting learner weights and memory...')\n",
    "            epsilon = 1\n",
    "            agent.build_models()\n",
    "            stuck = 0\n",
    "            agent.memory.reset()\n",
    "            last_reset = i_episode\n",
    "            # agent.actor_local.normalize(0.1)\n",
    "            # agent.actor_target.normalize(0.05)\n",
    "            # agent.critic_local.normalize(0.1)\n",
    "            # agent.critic_target.normalize(0.05)\n",
    "    if last_max > 30 and epsilon < 0.1:\n",
    "        epsilon = min(epsilon+0.01, 1)\n",
    "    if stop_train:\n",
    "        done = False\n",
    "\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    \n",
    "\n",
    "    noise.reset()\n",
    "    agent.reset_episode(state, learning, 1, True)\n",
    "    epoch = 0\n",
    "    \n",
    "    rewards = []\n",
    "    agent_actions = []\n",
    "#     for epoch in range(epochs):\n",
    "    while True:\n",
    "        \n",
    "        agent_action = agent.act(state)[0]\n",
    "        agent_actions.append(agent_action)\n",
    "        # if acting:\n",
    "        action = np.clip((1 - epsilon) * agent_action + epsilon * noise.sample(), -1, 1)\n",
    "        # else:\n",
    "            # action = noise.sample()\n",
    "        if epoch > epochs:\n",
    "            # remove bad episode from memory\n",
    "            # for pops in range(epoch):\n",
    "            #     agent.memory.memory.pop()\n",
    "            break\n",
    "#             action = agent_action + noise.sample() * max(epsilon,0.01)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "#         reward = done * 100 - 1\n",
    "        next_state = preprocess_state(next_state)\n",
    "        rewards.append(reward)\n",
    "        agent.step(state, action, reward, next_state, done, learning)\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        total_epochs += 1\n",
    "        epoch += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    noise.reset()\n",
    "    final_reward = np.sum(rewards)\n",
    "    \n",
    "    num_epochs = len(rewards)\n",
    "    mean_action = np.mean(agent_actions)\n",
    "    std_action = np.std(agent_actions)\n",
    "    rewards_hist.append(final_reward)\n",
    "\n",
    "    if len(rewards_hist) > 30:\n",
    "        mean_hist = np.mean(rewards_hist[len(rewards_hist)-20:])\n",
    "        # early stop if mean of 15 episodes > 90\n",
    "\n",
    "        if mean_hist > max_reward:\n",
    "            max_reward = mean_hist\n",
    "            last_max = 0\n",
    "        else:\n",
    "            last_max += 1\n",
    "\n",
    "        if mean_hist > 90:\n",
    "            stop_train = True\n",
    "            if np.mean(rewards_hist[len(rewards_hist)-30:]) > 90:\n",
    "                break\n",
    "        if mean_hist < 83:\n",
    "            stop_train = False\n",
    "        if mean_hist < 60 and mem_full:\n",
    "            stuck += 1\n",
    "\n",
    "        else:\n",
    "            stuck = min(stuck - 1, 0)\n",
    "\n",
    "    else:\n",
    "        mean_hist = np.mean(rewards_hist)\n",
    "    \n",
    "    \n",
    "\n",
    "    if i_episode % 1 == 0:\n",
    "        print(\"Ep:{:3d} R:{:4.2f} epo:{:4d}/{:4d} mu:{:1.3f} sd:{:1.3f} eps:{:1.3f} mem:{:5d} d:{} hs:{:4.2f} mrh: {:4.2f}\".\n",
    "              format(i_episode, final_reward, num_epochs, epochs, mean_action,\n",
    "               std_action, epsilon, mem_len, done, mean_hist, max_reward))\n",
    "\n",
    "\n",
    "# testing the model\n",
    "input('lets take a look at the model')\n",
    "tests = 10\n",
    "for i_episode in range(tests):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "\n",
    "    noise.reset()\n",
    "    agent.reset_episode(state, learning, 10, done)\n",
    "    epoch = 0\n",
    "    rewards = []\n",
    "    agent_actions = []\n",
    "    #     for epoch in range(epochs):\n",
    "    while True:\n",
    "        \n",
    "        agent_action = agent.act(state)[0]\n",
    "        agent_actions.append(agent_action)\n",
    "        # if acting:\n",
    "        action = [np.clip(agent_action, -1, 1)]\n",
    "        # else:\n",
    "            # action = noise.sample()\n",
    "        if epoch > 3000:\n",
    "            break\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "    #         reward = done * 100 - 1\n",
    "        next_state = preprocess_state(next_state)\n",
    "        rewards.append(reward)\n",
    "        agent.step(state, action, reward, next_state, done, learning)\n",
    "        state = next_state\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        total_epochs += 1\n",
    "        epoch += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    final_reward = np.sum(rewards)\n",
    "    num_epochs = len(rewards)\n",
    "    mean_action = np.mean(agent_actions)\n",
    "    std_action = np.std(agent_actions)\n",
    "    \n",
    "    print(\"Ep:{:3d} R:{:4.2f} epo:{:4d}/{:4d} mu:{:1.3f} sd:{:1.3f} eps:{:1.3f} d:{} \".format(i_episode, \n",
    "        final_reward, epoch, epochs, mean_action, std_action, epsilon, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = agent.critic_local.model.predict_on_batch([states,actions])\n",
    "# samples = [np.array(s).reshape(-1,2) for s in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = np.random.uniform(-1.2, 0.6, 1000)\n",
    "b = np.random.uniform(-0.07, 0.07, 1000)\n",
    "actions = np.repeat([-0.3,0.3],500).astype(np.float32).reshape(-1, 1)\n",
    "states = np.array((a,b)).T\n",
    "acts = agent.actor_target.model.predict_on_batch(np.array([preprocess_state(state) for state in states]))\n",
    "plt.hist(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = agent.actor_target.model.predict_on_batch(np.array([preprocess_state(state) for state in states]))\n",
    "plt.hist(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.concatenate((states,q),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, y_0, q_0 = [[x] for x,y,q in vals[0::2]], [[y] for x,y,q in vals[0::2]], [[q] for x,y,q in vals[0::2]]\n",
    "x_1, y_1, q_1 = [[x] for x,y,q in vals[1::2]], [[y] for x,y,q in vals[1::2]], [[q] for x,y,q in vals[1::2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0[np.argmin(x_0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=x_0, y=y_0, c=q_0, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=x_1, y=y_1, c=q_1, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env = env.unwrapped\n",
    "state = env.reset()\n",
    "state = preprocess_state(state)\n",
    "learning = False\n",
    "env.seed(2018)\n",
    "agent.reset_episode(state, learning, 1)\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "#     while True:\n",
    "\n",
    "#     agent_action = agent.act(state)\n",
    "    agent_action = agent.act(state)\n",
    "    agent_actions.append(agent_action)\n",
    "    action = agent_action\n",
    "\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = preprocess_state(next_state)\n",
    "    agent.step(state, action, reward, next_state, done, learning)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    x = x_0,\n",
    "    y = y_0,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color = q_0, #set color equal to a variable\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    )\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    x = x_1,\n",
    "    y = y_1,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color = q_1, #set color equal to a variable\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    )\n",
    ")\n",
    "py.iplot([trace0], filename='scatter-plot-with-colorscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "py.iplot([trace1], filename='scatter-plot-with-colorscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df= pd.DataFrame(vals[::2], columns=list('XYZ'))\n",
    "df2= pd.DataFrame(vals[1::2], columns=list('XYZ'))\n",
    "\n",
    "fig, (ax, ax2)=plt.subplots(ncols=2)\n",
    "ax.set_title(\"-.3\")\n",
    "ax.tricontourf(df[\"X\"], df[\"Y\"], df[\"Z\"])\n",
    "ax2.set_title(\"+.3\")\n",
    "ax2.tricontourf(df2[\"X\"], df2[\"Y\"], df2[\"Z\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.DataFrame(vals[1::2], columns=list('XYZ'))\n",
    "\n",
    "fig, (ax, ax2)=plt.subplots(ncols=2)\n",
    "ax.set_title(\"tripcolor\")\n",
    "ax.tripcolor(df[\"X\"], df[\"Y\"], df[\"Z\"])\n",
    "ax2.set_title(\"tricontour\")\n",
    "ax2.tricontourf(df[\"X\"], df[\"Y\"], df[\"Z\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
